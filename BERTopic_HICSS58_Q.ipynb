{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyrealQ/Twitter-Perceptions-Esports-2023-Asian-Games_HICSS-58/blob/main/BERTopic_HICSS58_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Credits**\n",
        "\n",
        "This project was inspired by and based on the code available at https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing\n",
        "\n",
        "```bibtex\n",
        "@article{grootendorst2022bertopic,\n",
        "  title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},\n",
        "  author={Grootendorst, Maarten},\n",
        "  journal={arXiv preprint arXiv:2203.05794},\n",
        "  year={2022}\n",
        "```"
      ],
      "metadata": {
        "id": "YdCjwvzRKTo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dependencies**"
      ],
      "metadata": {
        "id": "6KxLLolxoRwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDqjxQ_uiwt5"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic sentence_transformers adjustText openai tiktoken\n",
        "\n",
        "# DataMapPlot\n",
        "!git clone https://github.com/TutteInstitute/datamapplot.git\n",
        "!pip install datamapplot/."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies for GPU-accelerated HDBSCAN + UMAP"
      ],
      "metadata": {
        "id": "M2Rf25p1y0-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cudf-cu12 dask-cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cugraph-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cupy-cuda12x -f https://pip.cupy.dev/aarch64"
      ],
      "metadata": {
        "id": "GpSLvZnyy3mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data**"
      ],
      "metadata": {
        "id": "R5MZdcOJi_8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('YOUR FILE PATH')"
      ],
      "metadata": {
        "id": "CAYCqO5mjDnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tweet preprocessing 1: Lowercase, remove URLs/symbols/numbers/stopwords, tokenize"
      ],
      "metadata": {
        "id": "6m3LhEjX8vgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def pre_process(sentence):\n",
        "    # Convert to lowercase and strip leading/trailing whitespace\n",
        "    sentence = str(sentence).lower().strip()\n",
        "\n",
        "    # Remove URLs\n",
        "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n",
        "    sentence = re.sub(r'http[s]?://\\S+', '', sentence)\n",
        "    sentence = re.sub(r'\\S+\\.\\S+', '', sentence)\n",
        "\n",
        "    # Remove spcial symbols\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "\n",
        "    # Remove numbers\n",
        "    sentence = re.sub(r'\\d+', '', sentence)\n",
        "\n",
        "    # Define additional stopwords\n",
        "    my_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    valid_words = [word for word in words if word not in my_stopwords and len(word) > 1]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    return ' '.join(valid_words)\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "df['text1'] = df['text'].apply(pre_process)\n",
        "\n",
        "print('Jobs Done')"
      ],
      "metadata": {
        "id": "YnmbQeJujLDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tweet preprocessing 2: Remove URLs/symbols/numbers, condense spaces"
      ],
      "metadata": {
        "id": "G96RMLZ4JueN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "def pre_process(sentence):\n",
        "    # Remove URLs\n",
        "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n",
        "    sentence = re.sub(r'http[s]?://\\S+', '', sentence)\n",
        "    sentence = re.sub(r'\\S+\\.\\S+', '', sentence)\n",
        "\n",
        "    # Remove special symbols\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "\n",
        "    # Remove numbers\n",
        "    sentence = re.sub(r'\\d+', '', sentence)\n",
        "\n",
        "    # Condense all multiple spaces to a single space\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "\n",
        "    # Return the cleaned sentence\n",
        "    return sentence\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "df['text2'] = df['text'].apply(pre_process)\n",
        "\n",
        "print('Jobs Done')"
      ],
      "metadata": {
        "id": "_A_5ILTs9xaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.at[3082, 'text'])\n",
        "print(df.at[3082, 'text1'])\n",
        "print(df.at[3082, 'text2'])"
      ],
      "metadata": {
        "id": "TpFtqXg9jL1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5e46c2-d102-48cb-b694-b69a2f04bca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "team korea picks another gold taekwondo judo team turns away disappointment asian_games hangzhou_asian_games team_korea taekwondo judo fencing esports 항저우_아시안_게임 팀코리아 태권도 유도 펜싱 arirang_news 아리랑뉴스\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame with both original and cleaned text into a new Excel file\n",
        "df.to_excel('YOUR FILE PATH', index=False)"
      ],
      "metadata": {
        "id": "8EIXraUK8321"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Prompt Template**"
      ],
      "metadata": {
        "id": "LSE1heYCjcMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we can directly prompt the model, there is actually a template that we need to follow. The template looks as follows:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "\n",
        "{{ System Prompt }}\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "{{ User Prompt }} [/INST]\n",
        "\n",
        "{{ Model Answer }}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "This template consists of two main components, namely the `{{ System Prompt }}` and the `{{ User Prompt }}`:\n",
        "* The `{{ System Prompt }}` helps us guide the model during a conversation. For example, we can say that it is a helpful assisant that is specialized in labeling topics.\n",
        "* The  `{{ User Prompt }}` is where we ask it a question.\n",
        "\n",
        "You might have noticed the `[INST]` tags, these are used to identify the beginning and end of a prompt. We can use these to model the conversation history as we will see more in-depth later on.\n",
        "\n",
        "Next, let's see how we can use this template to optimize Llama 2 for topic modeling."
      ],
      "metadata": {
        "id": "tE6pKUJ0j1IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template\n",
        "\n",
        "We are going to keep our `system prompt` simple and to the point:"
      ],
      "metadata": {
        "id": "QCm-wENoj44J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt describes information given to all conversations\n",
        "system_prompt = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant for labeling topics.\n",
        "<</SYS>>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wFMPDZNaj1u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will tell the model that it is simply a helpful assistant for labeling topics since that is our main goal.\n",
        "\n",
        "In contrast, our `user prompt` is going to the be a bit more involved. It will consist of two components, an **example** and the **main prompt**.\n",
        "\n",
        "Let's start with the **example**. Most LLMs do a much better job of generating accurate responses if you give them an example to work with. We will show it an accurate example of the kind of output we are expecting."
      ],
      "metadata": {
        "id": "fyfkigJJkBom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt demonstrating the output we are looking for\n",
        "example_prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
        "- Meat, but especially beef, is the word food in terms of emissions.\n",
        "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
        "\n",
        "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "\n",
        "[/INST] Environmental impacts of eating meat\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ee5ubeSZkCHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example, based on a number of keywords and documents primarily about the impact of\n",
        "meat, helps to model to understand the kind of output it should give. We show the model that we were expecting only the label, which is easier for us to extract.\n",
        "\n",
        "Next, we will create a template that we can use within BERTopic:"
      ],
      "metadata": {
        "id": "BaVwyg47kGFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
        "main_prompt = \"\"\"\n",
        "[INST]\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "\n",
        "The topic is described by the following keywords: '[KEYWORDS]'.\n",
        "\n",
        "Based on the information about the topic above, create a short label of this topic, ensuring comprehension across languages. Make sure you to only return the label and nothing more.\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UXlecZKSkHCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two BERTopic-specific tags that are of interest, namely `[DOCUMENTS]` and `[KEYWORDS]`:\n",
        "\n",
        "* `[DOCUMENTS]` contain the top 5 most relevant documents to the topic\n",
        "* `[KEYWORDS]` contain the top 10 most relevant keywords to the topic as generated through c-TF-IDF\n",
        "\n",
        "This template will be filled accordingly to each topic. And finally, we can combine this into our final prompt:"
      ],
      "metadata": {
        "id": "DbDN9nknkJyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = system_prompt + example_prompt + main_prompt"
      ],
      "metadata": {
        "id": "U9WYhLoCkLzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERTopic**"
      ],
      "metadata": {
        "id": "U9Hf-Y2cktAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start with topic modeling, we will first need to perform two steps:\n",
        "* Pre-calculating Embeddings\n",
        "* Defining Sub-models"
      ],
      "metadata": {
        "id": "ELHXflhfkydB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing embeddings\n",
        "\n",
        "By pre-calculating the embeddings for each document, we can speed-up additional exploration steps and use the embeddings to quickly iterate over BERTopic's hyperparameters if needed.\n",
        "\n",
        "**TIP**: You can find a great overview of good embeddings for clustering on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)."
      ],
      "metadata": {
        "id": "NuktgsxDk2q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-calculate embeddings BAAI/bge-small-en-v1.5, BAAI/bge-small-en OR sentence-transformers/all-MiniLM-L6-v2 OR sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embeddings = embedding_model.encode(df['text1'].tolist(), show_progress_bar=True)"
      ],
      "metadata": {
        "id": "5cVndybGk4tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sub-models\n",
        "\n",
        "Next, we will define all sub-models in BERTopic and do some small tweaks to the number of clusters to be created, setting random states, etc."
      ],
      "metadata": {
        "id": "lVpICupXk6GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.cluster import HDBSCAN\n",
        "from cuml.manifold import UMAP\n",
        "\n",
        "# Create instances of GPU-accelerated UMAP and HDBSCAN\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', gen_min_span_tree=True, prediction_data=True)\n",
        "\n",
        "#from umap import UMAP\n",
        "#from hdbscan import HDBSCAN\n",
        "\n",
        "#umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "#hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ],
      "metadata": {
        "id": "yqbruaExk8q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a small bonus, we are going to reduce the embeddings we created before to 2-dimensions so that we can use them for visualization purposes when we have created our topics."
      ],
      "metadata": {
        "id": "MT8tYWS-lAsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-reduce embeddings for visualization purposes\n",
        "reduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "jXPLlQmClB3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representation models\n",
        "\n",
        "One of the ways we are going to represent the topics is with LLMs which should give us a nice label. However, we might want to have additional representations to view a topic from multiple angles.\n",
        "\n",
        "Here, we will be using c-TF-IDF as our main representation and [KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired), [MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance), and [GPT-4](https://openai.com/gpt-4) as our additional representations."
      ],
      "metadata": {
        "id": "7y5kGz06lFm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT4 text generator\n",
        "import openai\n",
        "import tiktoken\n",
        "from openai import Client\n",
        "from bertopic.representation import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('YOUR OPENAI KEY')\n",
        "client = Client(api_key=api_key)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer= tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "# Create your representation model\n",
        "GPT4 = OpenAI(\n",
        "    client,\n",
        "    prompt=prompt,\n",
        "    model=\"gpt-4o\",\n",
        "    delay_in_seconds=2,\n",
        "    chat=True,\n",
        "    nr_docs=10,\n",
        "    diversity=0.1,\n",
        "    doc_length=100,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "52HSYl-gkL6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration, LlamaCPP\n",
        "\n",
        "# KeyBERT\n",
        "keybert = KeyBERTInspired()\n",
        "\n",
        "# MMR\n",
        "mmr = MaximalMarginalRelevance(diversity=0.5)\n",
        "\n",
        "# All representation models\n",
        "representation_model = {\n",
        "    \"KeyBERT\": keybert,\n",
        "    \"MMR\": mmr,\n",
        "    \"GPT4\": GPT4\n",
        "}"
      ],
      "metadata": {
        "id": "0Z-pHNoQlKwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**\n",
        "\n",
        "Now that we have our models prepared, we can start training our topic model! We supply BERTopic with the sub-models of interest, run `.fit_transform`, and see what kind of topics we get."
      ],
      "metadata": {
        "id": "2ugZQVKdnT-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Train BERTopic with a custom CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2), min_df=10)\n",
        "\n",
        "topic_model = BERTopic(\n",
        "\n",
        "  # Sub-models\n",
        "  embedding_model=embedding_model,\n",
        "  vectorizer_model=vectorizer_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  representation_model=representation_model,\n",
        "\n",
        "  # Hyperparameters\n",
        "  calculate_probabilities=True,\n",
        "  verbose=True,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "topics, probs = topic_model.fit_transform(df['text1'])"
      ],
      "metadata": {
        "id": "0YwDtuHK3TwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we are done training our model, let's see what topics were generated:"
      ],
      "metadata": {
        "id": "qehsPl-Fo2uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_keywords = topic_model.get_topic_info()\n",
        "print(topic_keywords)"
      ],
      "metadata": {
        "id": "MRId--PUo5ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_documents(df['text1'], reduced_embeddings=reduced_embeddings,\n",
        "                                hide_document_hover=True, hide_annotations=True)"
      ],
      "metadata": {
        "id": "cpEySFMn9AHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier reduction"
      ],
      "metadata": {
        "id": "qbZIAyXZ6WoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the \"c-TF-IDF\" strategy with a threshold\n",
        "new_topics = topic_model.reduce_outliers(df['text1'], topics, strategy=\"c-tf-idf\", threshold=0.1)\n",
        "\n",
        "# Reduce all outliers that are left with the \"distributions\" strategy\n",
        "new_topics = topic_model.reduce_outliers(df['text1'], topics, strategy=\"distributions\")"
      ],
      "metadata": {
        "id": "Nuiyu-OQ6XQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.update_topics(df['text1'], topics=new_topics)"
      ],
      "metadata": {
        "id": "tasIncdF6ati"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_documents(df['text1'], reduced_embeddings=reduced_embeddings,\n",
        "                                hide_document_hover=True, hide_annotations=True)"
      ],
      "metadata": {
        "id": "EAV767dm_4s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_keywords.to_excel(\"YOUR FILE PATH\", index=False)"
      ],
      "metadata": {
        "id": "k6KS44Sgp1pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show topics for documents"
      ],
      "metadata": {
        "id": "VvXHXmqypk2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_document_info(df['text1'])"
      ],
      "metadata": {
        "id": "xSZxxhCEpmvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_info_output = topic_model.get_document_info(df['text1'])\n",
        "\n",
        "document_info_df = pd.DataFrame(document_info_output)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "document_info_df.to_excel(\"YOUR FILE PATH\", index=False)"
      ],
      "metadata": {
        "id": "ynK5E1Rmp-l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic probability distribution visualization for top N topics"
      ],
      "metadata": {
        "id": "LDoa-il9o8Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=10)"
      ],
      "metadata": {
        "id": "SpuI5oogo_cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic probability distribution visualization for each document"
      ],
      "metadata": {
        "id": "aTwd-lfGpB46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_distribution(topic_model.probabilities_[3082], min_probability=0.015)"
      ],
      "metadata": {
        "id": "2Q-AR05cpDvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intertopic distance map"
      ],
      "metadata": {
        "id": "pyrjxYZQqlz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()"
      ],
      "metadata": {
        "id": "3d7NyrsXqnsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heatmap"
      ],
      "metadata": {
        "id": "xFU8KteFFwxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "Ev_RRs4lFzWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical topic modeling"
      ],
      "metadata": {
        "id": "XEPNm2JXqttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster import hierarchy as sch\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Hierarchical topics\n",
        "linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
        "hierarchical_topics = topic_model.hierarchical_topics(df['text1'], linkage_function=linkage_function)"
      ],
      "metadata": {
        "id": "C4wgnQNXqvcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
      ],
      "metadata": {
        "id": "PamWyWngqxfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
        "print(tree)"
      ],
      "metadata": {
        "id": "vs3Cn3zeqz7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTopic has a reduce_topics method that uses the existing model information to do a topic reduction."
      ],
      "metadata": {
        "id": "JDapoc3PpJOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Further reduce topics\n",
        "topic_model.reduce_topics(df['text1'], nr_topics=9)\n",
        "\n",
        "# Get the list of topics\n",
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "7OYxWFn4pKJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If we would like to manually pick which topics to merge together based on domain knowledge, we can list the topic numbers and pass them into the merge_topics function."
      ],
      "metadata": {
        "id": "8GuKV9g9pRHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics_to_merge = [[0, 3],\n",
        "                   [2, 6]]\n",
        "topic_model.merge_topics(df['text1'], topics_to_merge)\n",
        "\n",
        "# Get the list of topics\n",
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "udydc1sDpSeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datamapplot visualization"
      ],
      "metadata": {
        "id": "IqCSuSd4q3Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datamapplot\n",
        "import re\n",
        "\n",
        "# Create a label for each document\n",
        "llm_labels = [re.sub(r'\\W+', ' ', label[0][0].split(\"\\n\")[0].replace('\"', '')) for label in topic_model.get_topics(full=True)[\"GPT4\"].values()]\n",
        "llm_labels = [label if label else \"Unlabelled\" for label in llm_labels]\n",
        "all_labels = [llm_labels[topic+topic_model._outliers] if topic != -1 else \"Unlabelled\" for topic in topics]\n",
        "\n",
        "# Run the visualization\n",
        "datamapplot.create_plot(\n",
        "    reduced_embeddings,\n",
        "    all_labels,\n",
        "    label_font_size=11,\n",
        "    title=\"2023 Asian Games Esports Discourse on X\",\n",
        "    sub_title=\"Topics labeled with `GPT-4`\",\n",
        "    label_wrap_width=20,\n",
        "    use_medoids=True\n",
        ")"
      ],
      "metadata": {
        "id": "PKWzvHXTq47C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topics over time"
      ],
      "metadata": {
        "id": "3q6OpSw-q_GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['tt'] = df['tt'].astype(str)\n",
        "df['text1'] = df['text1'].astype(str)\n",
        "timestamps = df.tt.to_list()\n",
        "topics_over_time = topic_model.topics_over_time(df['text1'], timestamps, nr_bins=30)"
      ],
      "metadata": {
        "id": "oY6Ot6oIrAWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics_over_time(topics_over_time, topics=[0, 5, 7, 8])"
      ],
      "metadata": {
        "id": "gJPCOGOKrB9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=15)"
      ],
      "metadata": {
        "id": "mGepT2mSrEO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}